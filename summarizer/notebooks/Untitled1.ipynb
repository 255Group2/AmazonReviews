{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3875db96-f844-458b-8a4b-8cd5b9e20a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:62: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/var/folders/tr/kxdyv4b52sj7yfvtgfyh4npw0000gq/T/ipykernel_13613/4010500244.py:62: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  self.punc = ''',.;:?!'\"()[]{}<>|\\/@#^&*_~=+\\n\\t—–-'''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training advanced model...\n",
      "Model trained on 129396 documents\n",
      "Vocabulary size: 220849\n",
      "Model saved to advanced_review_model.pkl\n",
      "Model loaded from advanced_review_model.pkl\n",
      "Vocabulary size: 220849\n",
      "\n",
      "Summary for new product:\n",
      "screen protector\n",
      "love the design\n",
      "great value for money\n",
      "this phone case\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import utils\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import jellyfish\n",
    "\n",
    "class AdvancedReviewSummarizerModel:\n",
    "    \"\"\"\n",
    "    Advanced model for review summarization that extracts meaningful phrases with context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"advanced_review_model.pkl\"):\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # Initialize core components\n",
    "        self.idf = {}\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Load NLTK resources\n",
    "        self._load_nltk_resources()\n",
    "        self._initialize_stopwords()\n",
    "        self._initialize_punctuation()\n",
    "        \n",
    "        # Parameters\n",
    "        self.SUMMARY_SIZE_FACTOR = 3\n",
    "        self.RF_WEIGHT = 2\n",
    "        self.LEVENSHTEIN_THRESHOLD = 0.85\n",
    "        \n",
    "        # Initialize semantic patterns\n",
    "        self._initialize_semantic_patterns()\n",
    "        \n",
    "    def _load_nltk_resources(self):\n",
    "        \"\"\"Download required NLTK resources if not present\"\"\"\n",
    "        nltk_downloader = nltk.downloader.Downloader()\n",
    "        resources = ['punkt', 'averaged_perceptron_tagger', 'stopwords', 'maxent_ne_chunker', 'words']\n",
    "        \n",
    "        for resource in resources:\n",
    "            if not nltk_downloader.is_installed(resource):\n",
    "                nltk_downloader.download(resource)\n",
    "    \n",
    "    def _initialize_stopwords(self):\n",
    "        \"\"\"Initialize the stopwords list\"\"\"\n",
    "        # Minimal stopwords for better phrase extraction\n",
    "        self.stop_words = set(['a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "                              'has', 'have', 'had', 'do', 'does', 'did', 'will', 'would',\n",
    "                              'can', 'could', 'should', 'i', 'me', 'my', 'we', 'our', 'you',\n",
    "                              'he', 'him', 'his', 'she', 'her', 'it', 'its', 'they', 'them',\n",
    "                              'this', 'that', 'these', 'those', 'and', 'or', 'but', 'in', 'on',\n",
    "                              'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "    \n",
    "    def _initialize_punctuation(self):\n",
    "        \"\"\"Initialize punctuation sets\"\"\"\n",
    "        self.punc = ''',.;:?!'\"()[]{}<>|\\/@#^&*_~=+\\n\\t—–-'''\n",
    "        self.fullstop = '.'\n",
    "    \n",
    "    def _initialize_semantic_patterns(self):\n",
    "        \"\"\"Initialize semantic patterns for phrase extraction\"\"\"\n",
    "        self.semantic_patterns = [\n",
    "            # Feature-quality patterns\n",
    "            (r'(?P<feature>audio|sound|battery|microphone|app|customer)\\s+(?P<quality>quality|life|interface|support)', 'FEATURE_QUALITY'),\n",
    "            \n",
    "            # Aspect-evaluation patterns\n",
    "            (r'(?P<aspect>volume|battery|price|design|build|screen)\\s+(?P<eval>is\\s+)?(?P<quality>great|good|excellent|poor|bad|high|low|enough|not\\s+enough)', 'ASPECT_EVALUATION'),\n",
    "            \n",
    "            # Problem-solution patterns\n",
    "            (r'(?P<problem>connection\\s+issues?|problems?|noise)\\s+(?P<action>fixed|resolved|solved)', 'PROBLEM_SOLUTION'),\n",
    "            \n",
    "            # Comfort-usage patterns\n",
    "            (r'(?P<comfort>comfortable|easy)\\s+to\\s+(?P<action>wear|use|hold|set\\s?up)', 'COMFORT_USAGE'),\n",
    "            \n",
    "            # Performance patterns\n",
    "            (r'(?P<device>microphone|camera)\\s+(?P<action>picks?\\s?up|captures?)\\s+(?P<object>(?:background\\s+)?noise|sound)', 'PERFORMANCE'),\n",
    "            \n",
    "            # Quality adjective patterns\n",
    "            (r'(?P<adjective>crisp|clear|sleek|premium|confusing)\\s+(?P<feature>audio|sound|design|interface|app)', 'QUALITY_ADJECTIVE'),\n",
    "            \n",
    "            # Durability patterns\n",
    "            (r'(?P<action>scratches?)\\s+(?P<manner>easily|quickly)', 'DURABILITY'),\n",
    "            \n",
    "            # Value patterns\n",
    "            (r'(?P<worth>worth\\s+the|high)\\s+(?P<aspect>price|cost|money)', 'VALUE'),\n",
    "            \n",
    "            # Satisfaction patterns\n",
    "            (r'(?P<feeling>love|like|hate)\\s+(?P<feature>the\\s+)?(?P<aspect>audio|battery|design|app|interface)', 'SATISFACTION'),\n",
    "            \n",
    "            # Comparison patterns\n",
    "            (r'(?P<comparison>better|worse)\\s+than\\s+(?P<reference>expected|advertised)', 'COMPARISON'),\n",
    "            \n",
    "            # Update patterns\n",
    "            (r'(?P<type>firmware|software)\\s+update\\s+(?P<result>fixed|solved|improved)', 'UPDATE_RESULT'),\n",
    "        ]\n",
    "        \n",
    "        # Compile patterns\n",
    "        self.compiled_patterns = [(re.compile(pattern, re.IGNORECASE), label) for pattern, label in self.semantic_patterns]\n",
    "    \n",
    "    def train(self, dataframe):\n",
    "        \"\"\"Train the model by computing IDF values\"\"\"\n",
    "        print(\"Training advanced model...\")\n",
    "        \n",
    "        vocabulary = set()\n",
    "        doc_f = defaultdict(lambda: 0)\n",
    "        \n",
    "        # Process each review\n",
    "        for i, row in dataframe.iterrows():\n",
    "            cleaned_review = self._text_preprocess_clean(row['all_reviews'])\n",
    "            vocabulary.update(cleaned_review)\n",
    "            \n",
    "            unique_words = set(cleaned_review)\n",
    "            for word in unique_words:\n",
    "                doc_f[word] += 1\n",
    "        \n",
    "        # Calculate IDF\n",
    "        DOC_COUNT = len(dataframe)\n",
    "        for word in vocabulary:\n",
    "            self.idf[word] = math.log10(DOC_COUNT / float(doc_f[word]))\n",
    "        \n",
    "        print(f\"Model trained on {DOC_COUNT} documents\")\n",
    "        print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def summarize_new_product(self, reviews):\n",
    "        \"\"\"Extract meaningful keyphrases using advanced semantic analysis\"\"\"\n",
    "        if isinstance(reviews, list):\n",
    "            all_reviews = ' '.join(reviews)\n",
    "        else:\n",
    "            all_reviews = reviews\n",
    "        \n",
    "        # Extract semantic phrases\n",
    "        semantic_phrases = self._extract_semantic_phrases(all_reviews)\n",
    "        \n",
    "        # Extract contextual phrases\n",
    "        contextual_phrases = self._extract_contextual_phrases(all_reviews)\n",
    "        \n",
    "        # Extract syntactic phrases\n",
    "        syntactic_phrases = self._extract_syntactic_phrases(all_reviews)\n",
    "        \n",
    "        # Combine and score all phrases\n",
    "        all_phrases = semantic_phrases + contextual_phrases + syntactic_phrases\n",
    "        \n",
    "        # Score phrases by importance and frequency\n",
    "        phrase_scores = self._score_phrases(all_phrases, all_reviews)\n",
    "        \n",
    "        # Get top 10 unique phrases\n",
    "        top_phrases = self._get_top_phrases(phrase_scores, 10)\n",
    "        \n",
    "        return top_phrases\n",
    "    \n",
    "    def _extract_semantic_phrases(self, text):\n",
    "        \"\"\"Extract phrases using semantic patterns\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for pattern, label in self.compiled_patterns:\n",
    "                matches = pattern.finditer(sentence)\n",
    "                for match in matches:\n",
    "                    # Extract the matched groups\n",
    "                    groups = match.groupdict()\n",
    "                    \n",
    "                    # Construct meaningful phrase based on pattern type\n",
    "                    if label == 'FEATURE_QUALITY':\n",
    "                        phrase = f\"{groups['feature']} {groups['quality']}\"\n",
    "                    elif label == 'ASPECT_EVALUATION':\n",
    "                        if 'quality' in groups:\n",
    "                            phrase = f\"{groups['aspect']} is {groups['quality']}\"\n",
    "                        else:\n",
    "                            phrase = f\"{groups['aspect']} {groups.get('eval', '')}\"\n",
    "                    elif label == 'PROBLEM_SOLUTION':\n",
    "                        phrase = f\"{groups['problem']} {groups['action']}\"\n",
    "                    elif label == 'COMFORT_USAGE':\n",
    "                        phrase = f\"{groups['comfort']} to {groups['action']}\"\n",
    "                    elif label == 'PERFORMANCE':\n",
    "                        phrase = f\"{groups['device']} {groups['action']} {groups['object']}\"\n",
    "                    elif label == 'QUALITY_ADJECTIVE':\n",
    "                        phrase = f\"{groups['adjective']} {groups['feature']}\"\n",
    "                    elif label == 'DURABILITY':\n",
    "                        phrase = f\"{groups['action']} {groups['manner']}\"\n",
    "                    elif label == 'VALUE':\n",
    "                        phrase = f\"{groups['worth']} {groups['aspect']}\"\n",
    "                    elif label == 'SATISFACTION':\n",
    "                        phrase = f\"{groups['feeling']} {groups.get('feature', '')} {groups['aspect']}\"\n",
    "                    elif label == 'COMPARISON':\n",
    "                        phrase = f\"{groups['comparison']} than {groups['reference']}\"\n",
    "                    elif label == 'UPDATE_RESULT':\n",
    "                        phrase = f\"{groups['type']} update {groups['result']}\"\n",
    "                    \n",
    "                    # Clean and validate phrase\n",
    "                    phrase = self._clean_phrase(phrase)\n",
    "                    if phrase:\n",
    "                        phrases.append((phrase, label))\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_contextual_phrases(self, text):\n",
    "        \"\"\"Extract phrases using contextual analysis\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            pos_tags = nltk.pos_tag(words)\n",
    "            \n",
    "            # Define important nouns\n",
    "            important_nouns = {'battery', 'audio', 'sound', 'quality', 'life', 'design', \n",
    "                              'microphone', 'app', 'interface', 'customer', 'support', \n",
    "                              'price', 'value', 'connection', 'volume', 'screen', \n",
    "                              'setup', 'update', 'noise', 'comfort', 'build'}\n",
    "            \n",
    "            for i, (word, tag) in enumerate(pos_tags):\n",
    "                if word in important_nouns:\n",
    "                    # Look for adjacent modifiers\n",
    "                    phrase_parts = [word]\n",
    "                    \n",
    "                    # Check left (adjectives/adverbs)\n",
    "                    for j in range(i-1, max(-1, i-3), -1):\n",
    "                        prev_word, prev_tag = pos_tags[j]\n",
    "                        if prev_tag.startswith(('JJ', 'RB')):\n",
    "                            phrase_parts.insert(0, prev_word)\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                    # Check right (nouns, prepositions)\n",
    "                    for j in range(i+1, min(len(pos_tags), i+3)):\n",
    "                        next_word, next_tag = pos_tags[j]\n",
    "                        if next_tag.startswith(('NN', 'IN')):\n",
    "                            phrase_parts.append(next_word)\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                    phrase = ' '.join(phrase_parts)\n",
    "                    phrase = self._clean_phrase(phrase)\n",
    "                    if phrase and len(phrase_parts) >= 2:\n",
    "                        phrases.append((phrase, 'CONTEXTUAL'))\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_syntactic_phrases(self, text):\n",
    "        \"\"\"Extract phrases using syntactic chunking\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Define chunking grammar\n",
    "        grammar = r\"\"\"\n",
    "        NP: {<DT|JJ|NN.*>+}          # chunk sequences of DT, JJ, NN\n",
    "        VP: {<VB.*><RB>*<NP>}        # verb + optional adverb + noun phrase\n",
    "        ADJ_NP: {<JJ><NN>}           # adjective + noun\n",
    "        \"\"\"\n",
    "        \n",
    "        cp = RegexpParser(grammar)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            pos_tags = nltk.pos_tag(words)\n",
    "            \n",
    "            # Parse the sentence\n",
    "            tree = cp.parse(pos_tags)\n",
    "            \n",
    "            # Extract noun phrases\n",
    "            for subtree in tree:\n",
    "                if isinstance(subtree, nltk.Tree):\n",
    "                    if subtree.label() in ['NP', 'ADJ_NP']:\n",
    "                        phrase = ' '.join(word for word, tag in subtree.leaves())\n",
    "                        phrase = self._clean_phrase(phrase)\n",
    "                        if phrase and len(phrase.split()) >= 2:\n",
    "                            phrases.append((phrase, 'SYNTACTIC'))\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _score_phrases(self, phrases, text):\n",
    "        \"\"\"Score phrases based on multiple factors\"\"\"\n",
    "        phrase_scores = defaultdict(float)\n",
    "        \n",
    "        for phrase, phrase_type in phrases:\n",
    "            # Base frequency score\n",
    "            frequency = text.lower().count(phrase.lower())\n",
    "            score = frequency * 2.0  # Base score multiplier\n",
    "            \n",
    "            # Type-based scoring\n",
    "            if phrase_type == 'FEATURE_QUALITY':\n",
    "                score *= 3.0  # Highest priority for feature-quality phrases\n",
    "            elif phrase_type in ['PROBLEM_SOLUTION', 'UPDATE_RESULT']:\n",
    "                score *= 2.5  # High priority for problems and solutions\n",
    "            elif phrase_type in ['PERFORMANCE', 'COMFORT_USAGE']:\n",
    "                score *= 2.0  # Medium priority for performance descriptions\n",
    "            elif phrase_type == 'SEMANTIC':\n",
    "                score *= 1.8\n",
    "            elif phrase_type == 'CONTEXTUAL':\n",
    "                score *= 1.5\n",
    "            else:\n",
    "                score *= 1.2\n",
    "            \n",
    "            # Length-based scoring\n",
    "            word_count = len(phrase.split())\n",
    "            if word_count == 2:\n",
    "                score *= 1.2  # Sweet spot for two-word phrases\n",
    "            elif word_count == 3:\n",
    "                score *= 1.1\n",
    "            \n",
    "            # Keyword importance\n",
    "            important_keywords = {'quality', 'battery', 'life', 'audio', 'sound', 'microphone', \n",
    "                                'connection', 'interface', 'support', 'customer', 'design', \n",
    "                                'comfort', 'volume', 'update', 'fixed', 'issue', 'problem'}\n",
    "            \n",
    "            for keyword in important_keywords:\n",
    "                if keyword in phrase.lower():\n",
    "                    score *= 1.3\n",
    "            \n",
    "            phrase_scores[phrase] = score\n",
    "        \n",
    "        return phrase_scores\n",
    "    \n",
    "    def _get_top_phrases(self, phrase_scores, n):\n",
    "        \"\"\"Get top N unique phrases\"\"\"\n",
    "        # Sort by score\n",
    "        sorted_phrases = sorted(phrase_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Remove overlapping phrases (keep the highest scoring)\n",
    "        final_phrases = []\n",
    "        seen_words = set()\n",
    "        \n",
    "        for phrase, score in sorted_phrases:\n",
    "            words = set(phrase.lower().split())\n",
    "            \n",
    "            # Check if this phrase is too similar to already selected ones\n",
    "            is_similar = False\n",
    "            for selected_phrase in final_phrases:\n",
    "                selected_words = set(selected_phrase.lower().split())\n",
    "                overlap = len(words.intersection(selected_words))\n",
    "                \n",
    "                # Consider phrases similar if they share more than 50% of words\n",
    "                if overlap > len(words) * 0.5 or overlap > len(selected_words) * 0.5:\n",
    "                    is_similar = True\n",
    "                    break\n",
    "            \n",
    "            if not is_similar and len(final_phrases) < n:\n",
    "                final_phrases.append(phrase)\n",
    "        \n",
    "        return final_phrases\n",
    "    \n",
    "    def _clean_phrase(self, phrase):\n",
    "        \"\"\"Clean and normalize a phrase\"\"\"\n",
    "        if not phrase:\n",
    "            return None\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
    "        \n",
    "        # Remove phrases that are too short or just punctuation\n",
    "        if len(phrase) < 3 or all(c in self.punc for c in phrase):\n",
    "            return None\n",
    "        \n",
    "        # Remove leading/trailing punctuation\n",
    "        phrase = phrase.strip(self.punc)\n",
    "        \n",
    "        # Normalize common variations\n",
    "        replacements = {\n",
    "            'apps': 'app',\n",
    "            'batteries': 'battery',\n",
    "            'interfaces': 'interface',\n",
    "            'noises': 'noise',\n",
    "            'connections': 'connection',\n",
    "            'microphones': 'microphone',\n",
    "            'issues': 'issue',\n",
    "            'problems': 'problem',\n",
    "            'updates': 'update'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            phrase = phrase.replace(old, new)\n",
    "        \n",
    "        return phrase if phrase else None\n",
    "    \n",
    "    def _text_preprocess_clean(self, review):\n",
    "        \"\"\"Clean and tokenize text for IDF calculation\"\"\"\n",
    "        # Basic cleaning\n",
    "        review = re.sub(r'[^\\w\\s]', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = re.sub(r'\\s+', ' ', review)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(review)\n",
    "        \n",
    "        # Remove very short tokens and numbers\n",
    "        tokens = [t for t in tokens if len(t) > 2 and not t.isdigit()]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        \"\"\"Save the trained model to disk\"\"\"\n",
    "        if path is None:\n",
    "            path = self.model_path\n",
    "            \n",
    "        model_data = {\n",
    "            'idf': self.idf,\n",
    "            'stop_words': self.stop_words,\n",
    "            'punc': self.punc,\n",
    "            'fullstop': self.fullstop,\n",
    "            'semantic_patterns': self.semantic_patterns,\n",
    "            'parameters': {\n",
    "                'SUMMARY_SIZE_FACTOR': self.SUMMARY_SIZE_FACTOR,\n",
    "                'RF_WEIGHT': self.RF_WEIGHT,\n",
    "                'LEVENSHTEIN_THRESHOLD': self.LEVENSHTEIN_THRESHOLD\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path=\"advanced_review_model.pkl\"):\n",
    "        \"\"\"Load a trained model from disk\"\"\"\n",
    "        model = cls(model_path=path)\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model.idf = model_data['idf']\n",
    "        model.stop_words = model_data['stop_words']\n",
    "        model.punc = model_data['punc']\n",
    "        model.fullstop = model_data['fullstop']\n",
    "        model.semantic_patterns = model_data['semantic_patterns']\n",
    "        \n",
    "        # Recompile patterns\n",
    "        model.compiled_patterns = [(re.compile(pattern, re.IGNORECASE), label) \n",
    "                                  for pattern, label in model.semantic_patterns]\n",
    "        \n",
    "        params = model_data['parameters']\n",
    "        model.SUMMARY_SIZE_FACTOR = params['SUMMARY_SIZE_FACTOR']\n",
    "        model.RF_WEIGHT = params['RF_WEIGHT']\n",
    "        model.LEVENSHTEIN_THRESHOLD = params['LEVENSHTEIN_THRESHOLD']\n",
    "        \n",
    "        print(f\"Model loaded from {path}\")\n",
    "        print(f\"Vocabulary size: {len(model.idf)}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    import utils\n",
    "    \n",
    "    # Training phase (one-time)\n",
    "    print(\"Loading training data...\")\n",
    "    file2 = os.path.join(\"data\", 'asin_numreviews_allreview.csv')\n",
    "    df_allreview = utils.csv_to_dataframe(file2)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = AdvancedReviewSummarizerModel()\n",
    "    model.train(df_allreview)\n",
    "    model.save()\n",
    "    \n",
    "    # Load trained model (for future use)\n",
    "    loaded_model = AdvancedReviewSummarizerModel.load()\n",
    "    \n",
    "    # Summarize new product reviews\n",
    "    new_reviews = [\n",
    "        \"This phone case is really good! It's durable and protective.\",\n",
    "        \"I love the design, but it's a bit bulky for my pocket.\",\n",
    "        \"Great value for money. The screen protector works perfectly.\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    summary = loaded_model.summarize_new_product(new_reviews)\n",
    "    print(\"\\nSummary for new product:\")\n",
    "    for phrase in summary:\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1fa2a-6b4d-4ee0-98e3-60c16db9cdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872a0fb-81ea-409b-bf46-b55eb49df506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25080a20-e6a2-459e-9e09-f8cf84d8f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from review_summarizer_model.pkl\n",
      "Vocabulary size: 194095\n",
      "\n",
      "Summary keyphrases:\n",
      "1. better performance\n",
      "2. support voice\n",
      "3. previous device\n",
      "4. android tablet\n",
      "5. better ventilation\n",
      "6. fast way\n",
      "7. everyday tasks\n",
      "8. resource heavy\n",
      "9. user manual\n",
      "10. voice commands\n"
     ]
    }
   ],
   "source": [
    "# For Untitled1.ipynb\n",
    "\n",
    "# Option 1: Simple solution - copy the class into your notebook\n",
    "# Copy the entire ReviewSummarizerModel class definition from the artifact above\n",
    "# and paste it directly into a cell in your notebook\n",
    "\n",
    "# OR\n",
    "\n",
    "\n",
    "# Once import works, use the model:\n",
    "try:\n",
    "    model = ReviewSummarizerModel.load(\"review_summarizer_model.pkl\")\n",
    "    \n",
    "    new_product_reviews = [\n",
    "    \"The product arrived two days late and the packaging was torn, but the item itself was undamaged.\",\n",
    "    \"Works flawlessly with my iPhone but refuses to pair with my Android tablet.\",\n",
    "    \"Feels durable and has survived a few accidental drops without any damage.\",\n",
    "    \"Interface is smooth and fast way better than my previous device.\",\n",
    "    \"Disappointed that it doesn’t support voice commands as advertised.\",\n",
    "    \"Great for everyday tasks but struggles with anything resource heavy.\",\n",
    "    \"Really appreciated the free accessories included in the box!\",\n",
    "    \"Overheats when used for more than an hour straight  needs better ventilation.\",\n",
    "    \"Setup was intuitive and the user manual was actually helpful.\",\n",
    "    \"Not bad overall, but I expected better performance for the price.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    summary = model.summarize_new_product(new_product_reviews)\n",
    "    \n",
    "    print(\"\\nSummary keyphrases:\")\n",
    "    for i, phrase in enumerate(summary, 1):\n",
    "        print(f\"{i}. {phrase}\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44810184-2e25-4bce-bf7d-b3bae9d44349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09937a63-7e08-4ff1-b313-e46e843e4682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c172c-119a-4855-b06b-ca722d7c963f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e699972-b180-4de1-a949-7ae2fac57909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9b3c6-369d-4606-80a0-370d835a7dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fbebf-3321-4b9e-adce-e4bdf812a723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f445b9-e707-402b-8f8a-a5bf8cc92a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b75755-b0e4-4d2b-90e9-b4a3e9d0fce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
