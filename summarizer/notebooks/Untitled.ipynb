{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf91b511-88ec-4a25-9fd4-978203213e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/spartan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/spartan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/spartan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/spartan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import utils\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "import jellyfish\n",
    "import spacy\n",
    "\n",
    "class SophisticatedReviewSummarizerModel:\n",
    "    \"\"\"\n",
    "    Sophisticated model for review summarization that understands context and extracts truly meaningful phrases.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"sophisticated_review_model.pkl\"):\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # Initialize core components\n",
    "        self.idf = {}\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Load NLTK resources\n",
    "        self._load_nltk_resources()\n",
    "        self._initialize_stopwords()\n",
    "        self._initialize_punctuation()\n",
    "        \n",
    "        # Parameters\n",
    "        self.SUMMARY_SIZE_FACTOR = 3\n",
    "        self.RF_WEIGHT = 2\n",
    "        self.LEVENSHTEIN_THRESHOLD = 0.85\n",
    "        \n",
    "        # Initialize semantic patterns and dependencies\n",
    "        self._initialize_semantic_patterns()\n",
    "        self._initialize_aspect_dictionaries()\n",
    "        \n",
    "        # Try to load spacy model (fallback if not available)\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except:\n",
    "            print(\"Spacy model not found. Some features may be limited.\")\n",
    "            self.nlp = None\n",
    "        \n",
    "    def _load_nltk_resources(self):\n",
    "        \"\"\"Download required NLTK resources if not present\"\"\"\n",
    "        nltk_downloader = nltk.downloader.Downloader()\n",
    "        resources = ['punkt', 'averaged_perceptron_tagger', 'stopwords', 'maxent_ne_chunker', 'words', 'wordnet']\n",
    "        \n",
    "        for resource in resources:\n",
    "            if not nltk_downloader.is_installed(resource):\n",
    "                nltk_downloader.download(resource)\n",
    "    \n",
    "    def _initialize_stopwords(self):\n",
    "        \"\"\"Initialize the stopwords list with product review specific words\"\"\"\n",
    "        # Minimal stopwords to preserve context\n",
    "        self.stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        \n",
    "        # Words to exclude from keyphrases\n",
    "        self.filter_words = set(['get', 'got', 'getting', 'gets', 'make', 'makes', 'making', 'made',\n",
    "                                'go', 'goes', 'going', 'went', 'come', 'comes', 'coming', 'came',\n",
    "                                'thing', 'things', 'stuff', 'bit', 'lot', 'very', 'really', 'just',\n",
    "                                'like', 'know', 'think', 'want', 'wanted', 'need', 'needed'])\n",
    "    \n",
    "    def _initialize_punctuation(self):\n",
    "        \"\"\"Initialize punctuation sets\"\"\"\n",
    "        self.punc = ''',.;:?!'\"()[]{}<>|\\/@#^&*_~=+\\n\\t—–-•'''\n",
    "        self.fullstop = '.'\n",
    "    \n",
    "    def _initialize_semantic_patterns(self):\n",
    "        \"\"\"Initialize semantic patterns for phrase extraction\"\"\"\n",
    "        # Enhanced semantic patterns with more specificity\n",
    "        self.semantic_patterns = [\n",
    "            # Product attributes with descriptors\n",
    "            (r'(?P<descriptor>crisp|clear|tinny|muffled|loud|soft|sharp)\\s+(?P<attribute>audio|sound|music|voice)', 'SOUND_QUALITY'),\n",
    "            (r'(?P<attribute>audio|sound|music)\\s+(?P<descriptor>quality|clarity|is\\s+(?:crisp|clear|muffled|tinny))', 'SOUND_QUALITY'),\n",
    "            \n",
    "            # Battery performance with context\n",
    "            (r'(?P<attribute>battery|charge)\\s+(?P<performance>life|lasts|duration)\\s+(?P<descriptor>great|amazing|excellent|poor|short|long|all\\s+day)', 'BATTERY_PERFORMANCE'),\n",
    "            (r'(?P<duration>all\\s+day|hours?|days?)\\s+(?P<performance>battery|charge|power)', 'BATTERY_DURATION'),\n",
    "            \n",
    "            # Connection and connectivity issues\n",
    "            (r'(?P<issue>connection|connectivity|pairing)\\s+(?P<problem>issues?|problems?|difficulties|dropped)', 'CONNECTION_ISSUES'),\n",
    "            (r'(?P<result>fixed|resolved|solved)\\s+(?P<issue>connection|connectivity|pairing)', 'CONNECTION_RESOLVED'),\n",
    "            \n",
    "            # Comfort and wearability\n",
    "            (r'(?P<comfort>comfortable|uncomfortable)\\s+to\\s+(?P<action>wear|use|hold)', 'COMFORT_LEVEL'),\n",
    "            (r'(?P<comfort>no|zero|minimal)\\s+(?P<issue>(?:ear\\s+)?fatigue|discomfort)', 'COMFORT_POSITIVE'),\n",
    "            \n",
    "            # User interface experiences\n",
    "            (r'(?P<interface>app|interface|software)\\s+(?P<quality>confusing|intuitive|user-friendly|difficult|easy)', 'INTERFACE_QUALITY'),\n",
    "            (r'(?P<missing>lacks?|missing)\\s+(?P<feature>basic\\s+features?|functionality)', 'INTERFACE_ISSUES'),\n",
    "            \n",
    "            # Customer support experiences\n",
    "            (r'(?P<service>customer\\s+support|tech\\s+support|service)\\s+(?P<quality>friendly|helpful|quick|slow|poor)', 'SUPPORT_QUALITY'),\n",
    "            (r'(?P<action>resolved?|fixed|helped)\\s+(?P<object>my\\s+)?(?P<issue>issue|problem)', 'SUPPORT_RESOLUTION'),\n",
    "            \n",
    "            # Price and value perception\n",
    "            (r'(?P<value>worth\\s+the|overpriced|expensive|cheap|affordable)\\s+(?P<aspect>price|cost|money)', 'VALUE_PERCEPTION'),\n",
    "            (r'(?P<price>price)\\s+(?P<evaluation>is\\s+)?(?P<adjective>high|low|fair|reasonable)', 'PRICE_EVALUATION'),\n",
    "            \n",
    "            # Durability and build quality\n",
    "            (r'(?P<action>scratches?|dents?|cracks?)\\s+(?P<manner>easily|quickly|readily)', 'DURABILITY_ISSUE'),\n",
    "            (r'(?P<quality>build|construction)\\s+(?P<evaluation>quality|is\\s+(?:good|poor|excellent|solid))', 'BUILD_QUALITY'),\n",
    "            \n",
    "            # Microphone performance\n",
    "            (r'(?P<device>microphone?|mic)\\s+(?P<action>picks?\\s?up|captures?)\\s+(?P<issue>background\\s+noise|ambient\\s+sounds?)', 'MIC_NOISE_ISSUE'),\n",
    "            (r'(?P<device>microphone?|mic)\\s+(?P<quality>quality|performance)\\s+(?P<evaluation>good|poor|excellent|bad)', 'MIC_QUALITY'),\n",
    "            \n",
    "            # Setup and installation\n",
    "            (r'(?P<process>setup|installation|configuration)\\s+(?P<evaluation>seamless|easy|difficult|complicated)', 'SETUP_EXPERIENCE'),\n",
    "            (r'(?P<evaluation>easy|hard|difficult)\\s+to\\s+(?P<action>set\\s?up|install|configure)', 'SETUP_EASE'),\n",
    "            \n",
    "            # Volume and loudness\n",
    "            (r'(?P<issue>volume|loudness)\\s+(?P<evaluation>not\\s+enough|insufficient|too\\s+low|perfect)\\s+(?P<context>outdoors?|in\\s+public)?', 'VOLUME_ISSUE'),\n",
    "            (r'(?P<evaluation>loud\\s+enough|too\\s+quiet|perfect\\s+volume)', 'VOLUME_EVALUATION'),\n",
    "            \n",
    "            # Software updates and fixes\n",
    "            (r'(?P<type>firmware|software)\\s+(?P<action>update)\\s+(?P<result>fixed|solved|improved|broke)', 'UPDATE_RESULT'),\n",
    "            (r'(?P<action>update)\\s+(?P<result>fixed|resolved)\\s+(?P<issue>everything|issues?|problems?)', 'UPDATE_FIX'),\n",
    "        ]\n",
    "        \n",
    "        # Compile patterns\n",
    "        self.compiled_patterns = [(re.compile(pattern, re.IGNORECASE), label) for pattern, label in self.semantic_patterns]\n",
    "    \n",
    "    def _initialize_aspect_dictionaries(self):\n",
    "        \"\"\"Initialize dictionaries for product aspects and their modifiers\"\"\"\n",
    "        # Product aspects and related words\n",
    "        self.product_aspects = {\n",
    "            'sound': ['audio', 'music', 'voice', 'volume', 'sound', 'noise'],\n",
    "            'battery': ['battery', 'charge', 'power', 'juice'],\n",
    "            'design': ['design', 'look', 'appearance', 'aesthetics', 'build'],\n",
    "            'comfort': ['comfort', 'fit', 'wear', 'ergonomic'],\n",
    "            'interface': ['app', 'interface', 'software', 'ui', 'menu'],\n",
    "            'support': ['support', 'service', 'help', 'assistance'],\n",
    "            'connectivity': ['connection', 'bluetooth', 'wireless', 'pairing', 'connect'],\n",
    "            'microphone': ['microphone', 'mic', 'calling', 'call'],\n",
    "            'durability': ['durability', 'build', 'quality', 'sturdy', 'solid'],\n",
    "            'setup': ['setup', 'installation', 'configuration', 'install'],\n",
    "            'price': ['price', 'cost', 'value', 'money', 'expensive', 'cheap'],\n",
    "            'performance': ['performance', 'speed', 'response', 'lag']\n",
    "        }\n",
    "        \n",
    "        # Sentiment modifiers\n",
    "        self.sentiment_modifiers = {\n",
    "            'positive': ['excellent', 'great', 'amazing', 'fantastic', 'superb', 'wonderful', \n",
    "                        'perfect', 'love', 'best', 'impressive', 'outstanding', 'superior'],\n",
    "            'negative': ['poor', 'bad', 'terrible', 'awful', 'horrible', 'worst', 'disappointing',\n",
    "                        'frustrating', 'annoying', 'hate', 'subpar', 'mediocre'],\n",
    "            'neutral': ['okay', 'fine', 'average', 'decent', 'adequate', 'acceptable']\n",
    "        }\n",
    "    \n",
    "    def summarize_new_product(self, reviews):\n",
    "        \"\"\"Extract meaningful keyphrases using sophisticated analysis\"\"\"\n",
    "        if isinstance(reviews, list):\n",
    "            all_reviews = ' '.join(reviews)\n",
    "        else:\n",
    "            all_reviews = reviews\n",
    "        \n",
    "        # Multiple extraction methods\n",
    "        semantic_phrases = self._extract_semantic_phrases(all_reviews)\n",
    "        dependency_phrases = self._extract_dependency_phrases(all_reviews) if self.nlp else []\n",
    "        contextual_phrases = self._extract_contextual_phrases(all_reviews)\n",
    "        compound_phrases = self._extract_compound_phrases(all_reviews)\n",
    "        \n",
    "        # Combine all phrases\n",
    "        all_phrases = semantic_phrases + dependency_phrases + contextual_phrases + compound_phrases\n",
    "        \n",
    "        # Advanced scoring and ranking\n",
    "        phrase_scores = self._advanced_score_phrases(all_phrases, all_reviews)\n",
    "        \n",
    "        # Extract top phrases with diversity\n",
    "        top_phrases = self._extract_diverse_phrases(phrase_scores, 10)\n",
    "        \n",
    "        return top_phrases\n",
    "    \n",
    "    def _extract_semantic_phrases(self, text):\n",
    "        \"\"\"Extract phrases using semantic patterns with improved context\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for pattern, label in self.compiled_patterns:\n",
    "                matches = pattern.finditer(sentence)\n",
    "                for match in matches:\n",
    "                    groups = match.groupdict()\n",
    "                    \n",
    "                    # Construct more meaningful phrases based on pattern\n",
    "                    if label == 'SOUND_QUALITY':\n",
    "                        if 'descriptor' in groups and 'attribute' in groups:\n",
    "                            phrase = f\"{groups['descriptor']} {groups['attribute']}\"\n",
    "                        else:\n",
    "                            phrase = f\"{groups['attribute']} {groups.get('descriptor', 'quality')}\"\n",
    "                    \n",
    "                    elif label == 'BATTERY_PERFORMANCE':\n",
    "                        if 'descriptor' in groups:\n",
    "                            phrase = f\"battery life {groups['descriptor']}\"\n",
    "                        else:\n",
    "                            phrase = f\"{groups['performance']} {groups.get('descriptor', '')}\"\n",
    "                    \n",
    "                    elif label == 'CONNECTION_ISSUES':\n",
    "                        phrase = f\"{groups['issue']} {groups['problem']}\"\n",
    "                    \n",
    "                    elif label == 'CONNECTION_RESOLVED':\n",
    "                        phrase = f\"{groups['issue']} {groups['result']}\"\n",
    "                    \n",
    "                    elif label == 'COMFORT_LEVEL':\n",
    "                        phrase = f\"{groups['comfort']} to {groups['action']}\"\n",
    "                    \n",
    "                    elif label == 'COMFORT_POSITIVE':\n",
    "                        phrase = f\"no {groups['issue']}\"\n",
    "                    \n",
    "                    elif label == 'INTERFACE_QUALITY':\n",
    "                        phrase = f\"app interface {groups['quality']}\"\n",
    "                    \n",
    "                    elif label == 'INTERFACE_ISSUES':\n",
    "                        phrase = f\"lacks basic features\"\n",
    "                    \n",
    "                    elif label == 'SUPPORT_QUALITY':\n",
    "                        phrase = f\"customer support {groups['quality']}\"\n",
    "                    \n",
    "                    elif label == 'SUPPORT_RESOLUTION':\n",
    "                        phrase = f\"support {groups['action']} issue\"\n",
    "                    \n",
    "                    elif label == 'VALUE_PERCEPTION':\n",
    "                        phrase = f\"{groups['value'].replace('_', ' ')} {groups['aspect']}\"\n",
    "                    \n",
    "                    elif label == 'DURABILITY_ISSUE':\n",
    "                        phrase = f\"{groups['action']} {groups['manner']}\"\n",
    "                    \n",
    "                    elif label == 'MIC_NOISE_ISSUE':\n",
    "                        phrase = f\"microphone picks up noise\"\n",
    "                    \n",
    "                    elif label == 'SETUP_EXPERIENCE':\n",
    "                        phrase = f\"setup {groups['evaluation']}\"\n",
    "                    \n",
    "                    elif label == 'VOLUME_ISSUE':\n",
    "                        if 'context' in groups and groups['context']:\n",
    "                            phrase = f\"volume {groups['evaluation']} {groups['context']}\"\n",
    "                        else:\n",
    "                            phrase = f\"volume {groups['evaluation']}\"\n",
    "                    \n",
    "                    elif label == 'UPDATE_RESULT':\n",
    "                        phrase = f\"{groups['type']} update {groups['result']}\"\n",
    "                    \n",
    "                    else:\n",
    "                        # Default construction\n",
    "                        phrase = ' '.join([v for v in groups.values() if v])\n",
    "                    \n",
    "                    # Clean and validate phrase\n",
    "                    phrase = self._clean_phrase(phrase)\n",
    "                    if phrase:\n",
    "                        phrases.append((phrase, label, 1.0))\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_dependency_phrases(self, text):\n",
    "        \"\"\"Extract phrases using dependency parsing (if spacy is available)\"\"\"\n",
    "        if not self.nlp:\n",
    "            return []\n",
    "        \n",
    "        phrases = []\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            # Find product aspects and their relations\n",
    "            for token in sent:\n",
    "                # Look for product aspect words\n",
    "                if token.lemma_ in sum(self.product_aspects.values(), []):\n",
    "                    # Get modifiers and complements\n",
    "                    modifiers = []\n",
    "                    complements = []\n",
    "                    \n",
    "                    for child in token.children:\n",
    "                        if child.dep_ in ['amod', 'compound']:  # adjective/compound modifiers\n",
    "                            modifiers.append(child.text)\n",
    "                        elif child.dep_ in ['prep', 'pobj']:  # prepositional phrases\n",
    "                            complements.append(child.text)\n",
    "                    \n",
    "                    # Construct phrases\n",
    "                    if modifiers:\n",
    "                        phrase = ' '.join(modifiers + [token.text])\n",
    "                        phrases.append((phrase, 'DEPENDENCY', 0.8))\n",
    "                    \n",
    "                    if complements and modifiers:\n",
    "                        phrase = ' '.join(modifiers + [token.text] + complements)\n",
    "                        phrases.append((phrase, 'DEPENDENCY', 0.9))\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_contextual_phrases(self, text):\n",
    "        \"\"\"Extract phrases with contextual understanding\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            pos_tags = nltk.pos_tag(words)\n",
    "            \n",
    "            # Advanced phrase construction\n",
    "            i = 0\n",
    "            while i < len(pos_tags):\n",
    "                word, tag = pos_tags[i]\n",
    "                \n",
    "                # Check if it's an important product aspect\n",
    "                for aspect, terms in self.product_aspects.items():\n",
    "                    if word in terms:\n",
    "                        phrase_parts = [word]\n",
    "                        context_found = False\n",
    "                        \n",
    "                        # Look ahead for relevant context\n",
    "                        j = i + 1\n",
    "                        while j < len(pos_tags) and j < i + 4:\n",
    "                            next_word, next_tag = pos_tags[j]\n",
    "                            \n",
    "                            # Add if it's a descriptive word\n",
    "                            if next_tag.startswith(('JJ', 'RB', 'VB')):\n",
    "                                phrase_parts.append(next_word)\n",
    "                                context_found = True\n",
    "                            elif next_tag.startswith('NN') and next_word in terms:\n",
    "                                phrase_parts.append(next_word)\n",
    "                                context_found = True\n",
    "                            elif next_word not in self.stop_words and not next_word in self.punc:\n",
    "                                # Check if it's semantically related\n",
    "                                for sentiment_set in self.sentiment_modifiers.values():\n",
    "                                    if next_word in sentiment_set:\n",
    "                                        phrase_parts.append(next_word)\n",
    "                                        context_found = True\n",
    "                                        break\n",
    "                            j += 1\n",
    "                        \n",
    "                        # If we found meaningful context, create phrase\n",
    "                        if context_found and len(phrase_parts) >= 2:\n",
    "                            phrase = ' '.join(phrase_parts)\n",
    "                            phrase = self._clean_phrase(phrase)\n",
    "                            if phrase:\n",
    "                                phrases.append((phrase, 'CONTEXTUAL', 0.7))\n",
    "                \n",
    "                i += 1\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_compound_phrases(self, text):\n",
    "        \"\"\"Extract compound phrases with better context understanding\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Find negation patterns\n",
    "            negation_words = ['not', 'no', 'never', 'hardly', 'barely', 'scarcely']\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            \n",
    "            # Look for negative phrases\n",
    "            for i, word in enumerate(words):\n",
    "                if word in negation_words and i < len(words) - 1:\n",
    "                    # Look for following adjectives or nouns\n",
    "                    for j in range(i + 1, min(i + 4, len(words))):\n",
    "                        next_word = words[j]\n",
    "                        if next_word not in self.stop_words:\n",
    "                            phrase = f\"{word} {next_word}\"\n",
    "                            \n",
    "                            # Extend with more context if available\n",
    "                            if j < len(words) - 1:\n",
    "                                further_word = words[j + 1]\n",
    "                                if further_word in sum(self.product_aspects.values(), []):\n",
    "                                    phrase += f\" {further_word}\"\n",
    "                            \n",
    "                            phrase = self._clean_phrase(phrase)\n",
    "                            if phrase:\n",
    "                                phrases.append((phrase, 'COMPOUND', 0.8))\n",
    "                            break\n",
    "            \n",
    "            # Look for comparison phrases\n",
    "            comparison_words = ['better', 'worse', 'best', 'worst', 'more', 'less']\n",
    "            for i, word in enumerate(words):\n",
    "                if word in comparison_words and i < len(words) - 2:\n",
    "                    if words[i + 1] == 'than':\n",
    "                        following = words[i + 2:]\n",
    "                        for aspect_terms in self.product_aspects.values():\n",
    "                            for term in aspect_terms:\n",
    "                                if term in following:\n",
    "                                    phrase = f\"{word} than {term}\"\n",
    "                                    phrase = self._clean_phrase(phrase)\n",
    "                                    if phrase:\n",
    "                                        phrases.append((phrase, 'COMPARISON', 0.9))\n",
    "                                    break\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _advanced_score_phrases(self, phrases, text):\n",
    "        \"\"\"Advanced scoring with multiple factors\"\"\"\n",
    "        phrase_scores = defaultdict(float)\n",
    "        \n",
    "        for phrase, phrase_type, base_weight in phrases:\n",
    "            # Calculate frequency\n",
    "            frequency = text.lower().count(phrase.lower())\n",
    "            \n",
    "            # Base score\n",
    "            score = frequency * base_weight * 2.0\n",
    "            \n",
    "            # Type-based multipliers\n",
    "            type_multipliers = {\n",
    "                'SOUND_QUALITY': 2.5,\n",
    "                'BATTERY_PERFORMANCE': 2.5,\n",
    "                'CONNECTION_ISSUES': 2.0,\n",
    "                'COMFORT_LEVEL': 2.0,\n",
    "                'INTERFACE_QUALITY': 2.0,\n",
    "                'SUPPORT_QUALITY': 1.8,\n",
    "                'MIC_NOISE_ISSUE': 2.2,\n",
    "                'VALUE_PERCEPTION': 1.9,\n",
    "                'UPDATE_RESULT': 2.3,\n",
    "                'DEPENDENCY': 1.5,\n",
    "                'CONTEXTUAL': 1.3,\n",
    "                'COMPOUND': 1.4\n",
    "            }\n",
    "            score *= type_multipliers.get(phrase_type, 1.0)\n",
    "            \n",
    "            # Word count bonus\n",
    "            word_count = len(phrase.split())\n",
    "            if word_count == 2:\n",
    "                score *= 1.3  # Optimal length\n",
    "            elif word_count == 3:\n",
    "                score *= 1.1\n",
    "            \n",
    "            # Aspect coverage bonus\n",
    "            covered_aspects = []\n",
    "            for aspect, terms in self.product_aspects.items():\n",
    "                if any(term in phrase.lower() for term in terms):\n",
    "                    covered_aspects.append(aspect)\n",
    "            \n",
    "            if covered_aspects:\n",
    "                score *= (1 + 0.2 * len(covered_aspects))\n",
    "            \n",
    "            # Sentiment clarity bonus\n",
    "            for sentiment, words in self.sentiment_modifiers.items():\n",
    "                if any(word in phrase.lower() for word in words):\n",
    "                    score *= 1.2\n",
    "                    break\n",
    "            \n",
    "            phrase_scores[phrase] = score\n",
    "        \n",
    "        return phrase_scores\n",
    "    \n",
    "    def _extract_diverse_phrases(self, phrase_scores, n):\n",
    "        \"\"\"Extract diverse phrases covering different aspects\"\"\"\n",
    "        sorted_phrases = sorted(phrase_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        final_phrases = []\n",
    "        covered_aspects = set()\n",
    "        \n",
    "        for phrase, score in sorted_phrases:\n",
    "            if len(final_phrases) >= n:\n",
    "                break\n",
    "            \n",
    "            # Check if this phrase covers a new aspect\n",
    "            phrase_aspects = set()\n",
    "            for aspect, terms in self.product_aspects.items():\n",
    "                if any(term in phrase.lower() for term in terms):\n",
    "                    phrase_aspects.add(aspect)\n",
    "            \n",
    "            # Add if it covers new aspects or is very high scoring\n",
    "            if not phrase_aspects.intersection(covered_aspects) or score > 5.0:\n",
    "                final_phrases.append(phrase)\n",
    "                covered_aspects.update(phrase_aspects)\n",
    "        \n",
    "        # If we don't have enough diverse phrases, fill with highest scoring ones\n",
    "        while len(final_phrases) < n and sorted_phrases:\n",
    "            phrase, _ = sorted_phrases.pop(0)\n",
    "            if phrase not in final_phrases:\n",
    "                final_phrases.append(phrase)\n",
    "        \n",
    "        return final_phrases\n",
    "    \n",
    "    def _clean_phrase(self, phrase):\n",
    "        \"\"\"Clean and normalize a phrase\"\"\"\n",
    "        if not phrase:\n",
    "            return None\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
    "        \n",
    "        # Remove artifacts\n",
    "        phrase = phrase.replace('_', ' ')\n",
    "        \n",
    "        # Remove leading/trailing punctuation\n",
    "        phrase = phrase.strip(self.punc)\n",
    "        \n",
    "        # Skip if too short or just stopwords\n",
    "        words = phrase.split()\n",
    "        if len(words) < 2 or all(w in self.stop_words for w in words):\n",
    "            return None\n",
    "        \n",
    "        # Remove phrases starting or ending with filter words\n",
    "        if words[0] in self.filter_words or words[-1] in self.filter_words:\n",
    "            return None\n",
    "        \n",
    "        return phrase\n",
    "    \n",
    "    def train(self, dataframe):\n",
    "        \"\"\"Train the model by computing IDF values\"\"\"\n",
    "        print(\"Training sophisticated model...\")\n",
    "        \n",
    "        vocabulary = set()\n",
    "        doc_f = defaultdict(lambda: 0)\n",
    "        \n",
    "        for i, row in dataframe.iterrows():\n",
    "            cleaned_review = self._text_preprocess_clean(row['all_reviews'])\n",
    "            vocabulary.update(cleaned_review)\n",
    "            \n",
    "            unique_words = set(cleaned_review)\n",
    "            for word in unique_words:\n",
    "                doc_f[word] += 1\n",
    "        \n",
    "        DOC_COUNT = len(dataframe)\n",
    "        for word in vocabulary:\n",
    "            self.idf[word] = math.log10(DOC_COUNT / float(doc_f[word]))\n",
    "        \n",
    "        print(f\"Model trained on {DOC_COUNT} documents\")\n",
    "        print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _text_preprocess_clean(self, review):\n",
    "        \"\"\"Clean and tokenize text for IDF calculation\"\"\"\n",
    "        review = re.sub(r'[^\\w\\s]', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = re.sub(r'\\s+', ' ', review)\n",
    "        \n",
    "        tokens = word_tokenize(review)\n",
    "        tokens = [t for t in tokens if len(t) > 2 and not t.isdigit()]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        \"\"\"Save the trained model to disk\"\"\"\n",
    "        if path is None:\n",
    "            path = self.model_path\n",
    "            \n",
    "        model_data = {\n",
    "            'idf': self.idf,\n",
    "            'stop_words': self.stop_words,\n",
    "            'filter_words': self.filter_words,\n",
    "            'punc': self.punc,\n",
    "            'fullstop': self.fullstop,\n",
    "            'semantic_patterns': self.semantic_patterns,\n",
    "            'product_aspects': self.product_aspects,\n",
    "            'sentiment_modifiers': self.sentiment_modifiers,\n",
    "            'parameters': {\n",
    "                'SUMMARY_SIZE_FACTOR': self.SUMMARY_SIZE_FACTOR,\n",
    "                'RF_WEIGHT': self.RF_WEIGHT,\n",
    "                'LEVENSHTEIN_THRESHOLD': self.LEVENSHTEIN_THRESHOLD\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path=\"sophisticated_review_model.pkl\"):\n",
    "        \"\"\"Load a trained model from disk\"\"\"\n",
    "        model = cls(model_path=path)\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model.idf = model_data['idf']\n",
    "        model.stop_words = model_data['stop_words']\n",
    "        model.filter_words = model_data['filter_words']\n",
    "        model.punc = model_data['punc']\n",
    "        model.fullstop = model_data['fullstop']\n",
    "        model.semantic_patterns = model_data['semantic_patterns']\n",
    "        model.product_aspects = model_data['product_aspects']\n",
    "        model.sentiment_modifiers = model_data['sentiment_modifiers']\n",
    "        \n",
    "        # Recompile patterns\n",
    "        model.compiled_patterns = [(re.compile(pattern, re.IGNORECASE), label) \n",
    "                                  for pattern, label in model.semantic_patterns]\n",
    "        \n",
    "        params = model_data['parameters']\n",
    "        model.SUMMARY_SIZE_FACTOR = params['SUMMARY_SIZE_FACTOR']\n",
    "        model.RF_WEIGHT = params['RF_WEIGHT']\n",
    "        model.LEVENSHTEIN_THRESHOLD = params['LEVENSHTEIN_THRESHOLD']\n",
    "        \n",
    "        print(f\"Model loaded from {path}\")\n",
    "        print(f\"Vocabulary size: {len(model.idf)}\")\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106f571-263f-4472-a348-02e91952986a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b9cd9-86a4-479b-806b-b2cbb976d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:67: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/var/folders/tr/kxdyv4b52sj7yfvtgfyh4npw0000gq/T/ipykernel_13605/4125327098.py:67: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  self.punc = ''',.;:?!'\"()[]{}<>|\\/@#^&*_~=+\\n\\t—–-•'''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training advanced model...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.parse import CoreNLPParser\n",
    "import jellyfish\n",
    "\n",
    "class AdvancedReviewSummarizerModel:\n",
    "    \"\"\"\n",
    "    Advanced model for review summarization without spacy dependency.\n",
    "    Uses NLTK for all NLP operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"advanced_review_model.pkl\"):\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # Initialize core components\n",
    "        self.idf = {}\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Load NLTK resources\n",
    "        self._load_nltk_resources()\n",
    "        self._initialize_stopwords()\n",
    "        self._initialize_punctuation()\n",
    "        \n",
    "        # Parameters\n",
    "        self.SUMMARY_SIZE_FACTOR = 3\n",
    "        self.RF_WEIGHT = 2\n",
    "        self.LEVENSHTEIN_THRESHOLD = 0.85\n",
    "        \n",
    "        # Initialize semantic patterns and dependencies\n",
    "        self._initialize_semantic_patterns()\n",
    "        self._initialize_aspect_dictionaries()\n",
    "        self._initialize_dependency_parser()\n",
    "        \n",
    "    def _load_nltk_resources(self):\n",
    "        \"\"\"Download required NLTK resources if not present\"\"\"\n",
    "        nltk_downloader = nltk.downloader.Downloader()\n",
    "        resources = ['punkt', 'averaged_perceptron_tagger', 'stopwords', 'maxent_ne_chunker', 'words', 'wordnet']\n",
    "        \n",
    "        for resource in resources:\n",
    "            if not nltk_downloader.is_installed(resource):\n",
    "                nltk_downloader.download(resource)\n",
    "    \n",
    "    def _initialize_stopwords(self):\n",
    "        \"\"\"Initialize the stopwords list with product review specific words\"\"\"\n",
    "        # Minimal stopwords to preserve context\n",
    "        self.stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        \n",
    "        # Words to exclude from keyphrases\n",
    "        self.filter_words = set(['get', 'got', 'getting', 'gets', 'make', 'makes', 'making', 'made',\n",
    "                                'go', 'goes', 'going', 'went', 'come', 'comes', 'coming', 'came',\n",
    "                                'thing', 'things', 'stuff', 'bit', 'lot', 'very', 'really', 'just',\n",
    "                                'like', 'know', 'think', 'want', 'wanted', 'need', 'needed'])\n",
    "    \n",
    "    def _initialize_punctuation(self):\n",
    "        \"\"\"Initialize punctuation sets\"\"\"\n",
    "        self.punc = ''',.;:?!'\"()[]{}<>|\\/@#^&*_~=+\\n\\t—–-•'''\n",
    "        self.fullstop = '.'\n",
    "    \n",
    "    def _initialize_semantic_patterns(self):\n",
    "        \"\"\"Initialize semantic patterns for phrase extraction\"\"\"\n",
    "        # Enhanced semantic patterns with more specificity\n",
    "        self.semantic_patterns = [\n",
    "            # Product attributes with descriptors\n",
    "            (r'(?P<descriptor>crisp|clear|tinny|muffled|loud|soft|sharp)\\s+(?P<attribute>audio|sound|music|voice)', 'SOUND_QUALITY'),\n",
    "            (r'(?P<attribute>audio|sound|music)\\s+(?P<descriptor>quality|clarity|is\\s+(?:crisp|clear|muffled|tinny))', 'SOUND_QUALITY'),\n",
    "            \n",
    "            # Battery performance with context\n",
    "            (r'(?P<attribute>battery|charge)\\s+(?P<performance>life|lasts|duration)\\s+(?P<descriptor>great|amazing|excellent|poor|short|long|all\\s+day)', 'BATTERY_PERFORMANCE'),\n",
    "            (r'(?P<duration>all\\s+day|hours?|days?)\\s+(?P<performance>battery|charge|power)', 'BATTERY_DURATION'),\n",
    "            \n",
    "            # Connection and connectivity issues\n",
    "            (r'(?P<issue>connection|connectivity|pairing)\\s+(?P<problem>issues?|problems?|difficulties|dropped)', 'CONNECTION_ISSUES'),\n",
    "            (r'(?P<result>fixed|resolved|solved)\\s+(?P<issue>connection|connectivity|pairing)', 'CONNECTION_RESOLVED'),\n",
    "            \n",
    "            # Comfort and wearability\n",
    "            (r'(?P<comfort>comfortable|uncomfortable)\\s+to\\s+(?P<action>wear|use|hold)', 'COMFORT_LEVEL'),\n",
    "            (r'(?P<comfort>no|zero|minimal)\\s+(?P<issue>(?:ear\\s+)?fatigue|discomfort)', 'COMFORT_POSITIVE'),\n",
    "            \n",
    "            # User interface experiences\n",
    "            (r'(?P<interface>app|interface|software)\\s+(?P<quality>confusing|intuitive|user-friendly|difficult|easy)', 'INTERFACE_QUALITY'),\n",
    "            (r'(?P<missing>lacks?|missing)\\s+(?P<feature>basic\\s+features?|functionality)', 'INTERFACE_ISSUES'),\n",
    "            \n",
    "            # Customer support experiences\n",
    "            (r'(?P<service>customer\\s+support|tech\\s+support|service)\\s+(?P<quality>friendly|helpful|quick|slow|poor)', 'SUPPORT_QUALITY'),\n",
    "            (r'(?P<action>resolved?|fixed|helped)\\s+(?P<object>my\\s+)?(?P<issue>issue|problem)', 'SUPPORT_RESOLUTION'),\n",
    "            \n",
    "            # Price and value perception\n",
    "            (r'(?P<value>worth\\s+the|overpriced|expensive|cheap|affordable)\\s+(?P<aspect>price|cost|money)', 'VALUE_PERCEPTION'),\n",
    "            (r'(?P<price>price)\\s+(?P<evaluation>is\\s+)?(?P<adjective>high|low|fair|reasonable)', 'PRICE_EVALUATION'),\n",
    "            \n",
    "            # Durability and build quality\n",
    "            (r'(?P<action>scratches?|dents?|cracks?)\\s+(?P<manner>easily|quickly|readily)', 'DURABILITY_ISSUE'),\n",
    "            (r'(?P<quality>build|construction)\\s+(?P<evaluation>quality|is\\s+(?:good|poor|excellent|solid))', 'BUILD_QUALITY'),\n",
    "            \n",
    "            # Microphone performance\n",
    "            (r'(?P<device>microphone?|mic)\\s+(?P<action>picks?\\s?up|captures?)\\s+(?P<issue>background\\s+noise|ambient\\s+sounds?)', 'MIC_NOISE_ISSUE'),\n",
    "            (r'(?P<device>microphone?|mic)\\s+(?P<quality>quality|performance)\\s+(?P<evaluation>good|poor|excellent|bad)', 'MIC_QUALITY'),\n",
    "            \n",
    "            # Setup and installation\n",
    "            (r'(?P<process>setup|installation|configuration)\\s+(?P<evaluation>seamless|easy|difficult|complicated)', 'SETUP_EXPERIENCE'),\n",
    "            (r'(?P<evaluation>easy|hard|difficult)\\s+to\\s+(?P<action>set\\s?up|install|configure)', 'SETUP_EASE'),\n",
    "            \n",
    "            # Volume and loudness\n",
    "            (r'(?P<issue>volume|loudness)\\s+(?P<evaluation>not\\s+enough|insufficient|too\\s+low|perfect)\\s+(?P<context>outdoors?|in\\s+public)?', 'VOLUME_ISSUE'),\n",
    "            (r'(?P<evaluation>loud\\s+enough|too\\s+quiet|perfect\\s+volume)', 'VOLUME_EVALUATION'),\n",
    "            \n",
    "            # Software updates and fixes\n",
    "            (r'(?P<type>firmware|software)\\s+(?P<action>update)\\s+(?P<result>fixed|solved|improved|broke)', 'UPDATE_RESULT'),\n",
    "            (r'(?P<action>update)\\s+(?P<result>fixed|resolved)\\s+(?P<issue>everything|issues?|problems?)', 'UPDATE_FIX'),\n",
    "        ]\n",
    "        \n",
    "        # Compile patterns\n",
    "        self.compiled_patterns = [(re.compile(pattern, re.IGNORECASE), label) for pattern, label in self.semantic_patterns]\n",
    "    \n",
    "    def _initialize_aspect_dictionaries(self):\n",
    "        \"\"\"Initialize dictionaries for product aspects and their modifiers\"\"\"\n",
    "        # Product aspects and related words\n",
    "        self.product_aspects = {\n",
    "            'sound': ['audio', 'music', 'voice', 'volume', 'sound', 'noise'],\n",
    "            'battery': ['battery', 'charge', 'power', 'juice'],\n",
    "            'design': ['design', 'look', 'appearance', 'aesthetics', 'build'],\n",
    "            'comfort': ['comfort', 'fit', 'wear', 'ergonomic'],\n",
    "            'interface': ['app', 'interface', 'software', 'ui', 'menu'],\n",
    "            'support': ['support', 'service', 'help', 'assistance'],\n",
    "            'connectivity': ['connection', 'bluetooth', 'wireless', 'pairing', 'connect'],\n",
    "            'microphone': ['microphone', 'mic', 'calling', 'call'],\n",
    "            'durability': ['durability', 'build', 'quality', 'sturdy', 'solid'],\n",
    "            'setup': ['setup', 'installation', 'configuration', 'install'],\n",
    "            'price': ['price', 'cost', 'value', 'money', 'expensive', 'cheap'],\n",
    "            'performance': ['performance', 'speed', 'response', 'lag']\n",
    "        }\n",
    "        \n",
    "        # Sentiment modifiers\n",
    "        self.sentiment_modifiers = {\n",
    "            'positive': ['excellent', 'great', 'amazing', 'fantastic', 'superb', 'wonderful', \n",
    "                        'perfect', 'love', 'best', 'impressive', 'outstanding', 'superior'],\n",
    "            'negative': ['poor', 'bad', 'terrible', 'awful', 'horrible', 'worst', 'disappointing',\n",
    "                        'frustrating', 'annoying', 'hate', 'subpar', 'mediocre'],\n",
    "            'neutral': ['okay', 'fine', 'average', 'decent', 'adequate', 'acceptable']\n",
    "        }\n",
    "    \n",
    "    def _initialize_dependency_parser(self):\n",
    "        \"\"\"Initialize NLTK-based dependency parser (alternative to spacy)\"\"\"\n",
    "        # Define chunk grammar for dependency-like relationships\n",
    "        self.chunk_grammar = r\"\"\"\n",
    "        NP: {<DT|JJ|NN.*>+}          # noun phrases\n",
    "        VP: {<VB.*><RB>*<NP>}        # verb phrases\n",
    "        PREP: {<IN><NP>}             # prepositional phrases\n",
    "        ADJ_NP: {<JJ><NN>}           # adjective + noun\n",
    "        SUBJ_PRED: {<NP><VB.*>}      # subject + predicate\n",
    "        \"\"\"\n",
    "        self.chunk_parser = RegexpParser(self.chunk_grammar)\n",
    "    \n",
    "    def summarize_new_product(self, reviews):\n",
    "        \"\"\"Extract meaningful keyphrases using advanced analysis\"\"\"\n",
    "        if isinstance(reviews, list):\n",
    "            all_reviews = ' '.join(reviews)\n",
    "        else:\n",
    "            all_reviews = reviews\n",
    "        \n",
    "        # Multiple extraction methods\n",
    "        semantic_phrases = self._extract_semantic_phrases(all_reviews)\n",
    "        dependency_phrases = self._extract_dependency_phrases(all_reviews)\n",
    "        contextual_phrases = self._extract_contextual_phrases(all_reviews)\n",
    "        compound_phrases = self._extract_compound_phrases(all_reviews)\n",
    "        linguistic_phrases = self._extract_linguistic_patterns(all_reviews)\n",
    "        \n",
    "        # Combine all phrases\n",
    "        all_phrases = semantic_phrases + dependency_phrases + contextual_phrases + compound_phrases + linguistic_phrases\n",
    "        \n",
    "        # Advanced scoring and ranking\n",
    "        phrase_scores = self._advanced_score_phrases(all_phrases, all_reviews)\n",
    "        \n",
    "        # Extract top phrases with diversity\n",
    "        top_phrases = self._extract_diverse_phrases(phrase_scores, 10)\n",
    "        \n",
    "        return top_phrases\n",
    "    \n",
    "    def _extract_dependency_phrases(self, text):\n",
    "        \"\"\"Extract phrases using NLTK chunking (spacy alternative)\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            pos_tags = nltk.pos_tag(words)\n",
    "            \n",
    "            # Parse using chunk grammar\n",
    "            tree = self.chunk_parser.parse(pos_tags)\n",
    "            \n",
    "            # Extract phrases from parsed tree\n",
    "            for subtree in tree:\n",
    "                if isinstance(subtree, nltk.Tree):\n",
    "                    # Extract phrases based on chunk type\n",
    "                    if subtree.label() in ['NP', 'ADJ_NP']:\n",
    "                        phrase = ' '.join(word for word, tag in subtree.leaves())\n",
    "                        \n",
    "                        # Check if it contains product aspects\n",
    "                        for aspect, terms in self.product_aspects.items():\n",
    "                            if any(term in phrase for term in terms):\n",
    "                                phrase = self._clean_phrase(phrase)\n",
    "                                if phrase:\n",
    "                                    phrases.append((phrase, 'DEPENDENCY', 0.8))\n",
    "                                break\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_linguistic_patterns(self, text):\n",
    "        \"\"\"Extract phrases using linguistic patterns\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            pos_tags = nltk.pos_tag(words)\n",
    "            \n",
    "            # Look for specific linguistic patterns\n",
    "            for i in range(len(pos_tags)):\n",
    "                word, tag = pos_tags[i]\n",
    "                \n",
    "                # Pattern 1: Adjective + Product Aspect\n",
    "                if tag.startswith('JJ') and i < len(pos_tags) - 1:\n",
    "                    next_word, next_tag = pos_tags[i + 1]\n",
    "                    if next_tag.startswith('NN'):\n",
    "                        for aspect, terms in self.product_aspects.items():\n",
    "                            if next_word in terms:\n",
    "                                phrase = f\"{word} {next_word}\"\n",
    "                                phrases.append((phrase, 'LINGUISTIC', 0.9))\n",
    "                \n",
    "                # Pattern 2: Product Aspect + Modifier\n",
    "                if tag.startswith('NN'):\n",
    "                    for aspect, terms in self.product_aspects.items():\n",
    "                        if word in terms:\n",
    "                            # Look for following modifiers\n",
    "                            if i < len(pos_tags) - 1:\n",
    "                                next_word, next_tag = pos_tags[i + 1]\n",
    "                                if next_tag.startswith(('JJ', 'RB', 'VB')):\n",
    "                                    phrase = f\"{word} {next_word}\"\n",
    "                                    phrases.append((phrase, 'LINGUISTIC', 0.85))\n",
    "                            \n",
    "                            # Look for preceding modifiers\n",
    "                            if i > 0:\n",
    "                                prev_word, prev_tag = pos_tags[i - 1]\n",
    "                                if prev_tag.startswith(('JJ', 'RB')):\n",
    "                                    phrase = f\"{prev_word} {word}\"\n",
    "                                    phrases.append((phrase, 'LINGUISTIC', 0.85))\n",
    "                \n",
    "                # Pattern 3: Verb + Product Aspect (e.g., \"scratches easily\")\n",
    "                if tag.startswith('VB') and i < len(pos_tags) - 1:\n",
    "                    next_word, next_tag = pos_tags[i + 1]\n",
    "                    if next_tag.startswith('RB'):\n",
    "                        phrase = f\"{word} {next_word}\"\n",
    "                        phrases.append((phrase, 'LINGUISTIC', 0.8))\n",
    "                \n",
    "                # Pattern 4: Negation + Adjective/Verb (e.g., \"not enough\")\n",
    "                if word in ['not', 'no', 'never'] and i < len(pos_tags) - 1:\n",
    "                    next_word, next_tag = pos_tags[i + 1]\n",
    "                    if next_tag.startswith(('JJ', 'RB', 'VB')):\n",
    "                        phrase = f\"{word} {next_word}\"\n",
    "                        phrases.append((phrase, 'LINGUISTIC', 0.9))\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_semantic_phrases(self, text):\n",
    "        \"\"\"Extract phrases using semantic patterns with improved context\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for pattern, label in self.compiled_patterns:\n",
    "                matches = pattern.finditer(sentence)\n",
    "                for match in matches:\n",
    "                    groups = match.groupdict()\n",
    "                    \n",
    "                    # Construct more meaningful phrases based on pattern\n",
    "                    if label == 'SOUND_QUALITY':\n",
    "                        if 'descriptor' in groups and 'attribute' in groups:\n",
    "                            phrase = f\"{groups['descriptor']} {groups['attribute']}\"\n",
    "                        else:\n",
    "                            phrase = f\"{groups['attribute']} {groups.get('descriptor', 'quality')}\"\n",
    "                    \n",
    "                    elif label == 'BATTERY_PERFORMANCE':\n",
    "                        if 'descriptor' in groups:\n",
    "                            phrase = f\"battery life {groups['descriptor']}\"\n",
    "                        else:\n",
    "                            phrase = f\"{groups['performance']} {groups.get('descriptor', '')}\"\n",
    "                    \n",
    "                    elif label == 'CONNECTION_ISSUES':\n",
    "                        phrase = f\"{groups['issue']} {groups['problem']}\"\n",
    "                    \n",
    "                    elif label == 'CONNECTION_RESOLVED':\n",
    "                        phrase = f\"{groups['issue']} {groups['result']}\"\n",
    "                    \n",
    "                    elif label == 'COMFORT_LEVEL':\n",
    "                        phrase = f\"{groups['comfort']} to {groups['action']}\"\n",
    "                    \n",
    "                    elif label == 'COMFORT_POSITIVE':\n",
    "                        phrase = f\"no {groups['issue']}\"\n",
    "                    \n",
    "                    elif label == 'INTERFACE_QUALITY':\n",
    "                        phrase = f\"app interface {groups['quality']}\"\n",
    "                    \n",
    "                    elif label == 'INTERFACE_ISSUES':\n",
    "                        phrase = f\"lacks basic features\"\n",
    "                    \n",
    "                    elif label == 'SUPPORT_QUALITY':\n",
    "                        phrase = f\"customer support {groups['quality']}\"\n",
    "                    \n",
    "                    elif label == 'SUPPORT_RESOLUTION':\n",
    "                        phrase = f\"support {groups['action']} issue\"\n",
    "                    \n",
    "                    elif label == 'VALUE_PERCEPTION':\n",
    "                        phrase = f\"{groups['value'].replace('_', ' ')} {groups['aspect']}\"\n",
    "                    \n",
    "                    elif label == 'DURABILITY_ISSUE':\n",
    "                        phrase = f\"{groups['action']} {groups['manner']}\"\n",
    "                    \n",
    "                    elif label == 'MIC_NOISE_ISSUE':\n",
    "                        phrase = f\"microphone picks up noise\"\n",
    "                    \n",
    "                    elif label == 'SETUP_EXPERIENCE':\n",
    "                        phrase = f\"setup {groups['evaluation']}\"\n",
    "                    \n",
    "                    elif label == 'VOLUME_ISSUE':\n",
    "                        if 'context' in groups and groups['context']:\n",
    "                            phrase = f\"volume {groups['evaluation']} {groups['context']}\"\n",
    "                        else:\n",
    "                            phrase = f\"volume {groups['evaluation']}\"\n",
    "                    \n",
    "                    elif label == 'UPDATE_RESULT':\n",
    "                        phrase = f\"{groups['type']} update {groups['result']}\"\n",
    "                    \n",
    "                    else:\n",
    "                        # Default construction\n",
    "                        phrase = ' '.join([v for v in groups.values() if v])\n",
    "                    \n",
    "                    # Clean and validate phrase\n",
    "                    phrase = self._clean_phrase(phrase)\n",
    "                    if phrase:\n",
    "                        phrases.append((phrase, label, 1.0))\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_contextual_phrases(self, text):\n",
    "        \"\"\"Extract phrases with contextual understanding\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            pos_tags = nltk.pos_tag(words)\n",
    "            \n",
    "            # Advanced phrase construction\n",
    "            i = 0\n",
    "            while i < len(pos_tags):\n",
    "                word, tag = pos_tags[i]\n",
    "                \n",
    "                # Check if it's an important product aspect\n",
    "                for aspect, terms in self.product_aspects.items():\n",
    "                    if word in terms:\n",
    "                        phrase_parts = [word]\n",
    "                        context_found = False\n",
    "                        \n",
    "                        # Look ahead for relevant context\n",
    "                        j = i + 1\n",
    "                        while j < len(pos_tags) and j < i + 4:\n",
    "                            next_word, next_tag = pos_tags[j]\n",
    "                            \n",
    "                            # Add if it's a descriptive word\n",
    "                            if next_tag.startswith(('JJ', 'RB', 'VB')):\n",
    "                                phrase_parts.append(next_word)\n",
    "                                context_found = True\n",
    "                            elif next_tag.startswith('NN') and next_word in terms:\n",
    "                                phrase_parts.append(next_word)\n",
    "                                context_found = True\n",
    "                            elif next_word not in self.stop_words and not next_word in self.punc:\n",
    "                                # Check if it's semantically related\n",
    "                                for sentiment_set in self.sentiment_modifiers.values():\n",
    "                                    if next_word in sentiment_set:\n",
    "                                        phrase_parts.append(next_word)\n",
    "                                        context_found = True\n",
    "                                        break\n",
    "                            j += 1\n",
    "                        \n",
    "                        # If we found meaningful context, create phrase\n",
    "                        if context_found and len(phrase_parts) >= 2:\n",
    "                            phrase = ' '.join(phrase_parts)\n",
    "                            phrase = self._clean_phrase(phrase)\n",
    "                            if phrase:\n",
    "                                phrases.append((phrase, 'CONTEXTUAL', 0.7))\n",
    "                \n",
    "                i += 1\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _extract_compound_phrases(self, text):\n",
    "        \"\"\"Extract compound phrases with better context understanding\"\"\"\n",
    "        phrases = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Find negation patterns\n",
    "            negation_words = ['not', 'no', 'never', 'hardly', 'barely', 'scarcely']\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            \n",
    "            # Look for negative phrases\n",
    "            for i, word in enumerate(words):\n",
    "                if word in negation_words and i < len(words) - 1:\n",
    "                    # Look for following adjectives or nouns\n",
    "                    for j in range(i + 1, min(i + 4, len(words))):\n",
    "                        next_word = words[j]\n",
    "                        if next_word not in self.stop_words:\n",
    "                            phrase = f\"{word} {next_word}\"\n",
    "                            \n",
    "                            # Extend with more context if available\n",
    "                            if j < len(words) - 1:\n",
    "                                further_word = words[j + 1]\n",
    "                                if further_word in sum(self.product_aspects.values(), []):\n",
    "                                    phrase += f\" {further_word}\"\n",
    "                            \n",
    "                            phrase = self._clean_phrase(phrase)\n",
    "                            if phrase:\n",
    "                                phrases.append((phrase, 'COMPOUND', 0.8))\n",
    "                            break\n",
    "            \n",
    "            # Look for comparison phrases\n",
    "            comparison_words = ['better', 'worse', 'best', 'worst', 'more', 'less']\n",
    "            for i, word in enumerate(words):\n",
    "                if word in comparison_words and i < len(words) - 2:\n",
    "                    if words[i + 1] == 'than':\n",
    "                        following = words[i + 2:]\n",
    "                        for aspect_terms in self.product_aspects.values():\n",
    "                            for term in aspect_terms:\n",
    "                                if term in following:\n",
    "                                    phrase = f\"{word} than {term}\"\n",
    "                                    phrase = self._clean_phrase(phrase)\n",
    "                                    if phrase:\n",
    "                                        phrases.append((phrase, 'COMPARISON', 0.9))\n",
    "                                    break\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def _advanced_score_phrases(self, phrases, text):\n",
    "        \"\"\"Advanced scoring with multiple factors\"\"\"\n",
    "        phrase_scores = defaultdict(float)\n",
    "        \n",
    "        for phrase, phrase_type, base_weight in phrases:\n",
    "            # Calculate frequency\n",
    "            frequency = text.lower().count(phrase.lower())\n",
    "            \n",
    "            # Base score\n",
    "            score = frequency * base_weight * 2.0\n",
    "            \n",
    "            # Type-based multipliers\n",
    "            type_multipliers = {\n",
    "                'SOUND_QUALITY': 2.5,\n",
    "                'BATTERY_PERFORMANCE': 2.5,\n",
    "                'CONNECTION_ISSUES': 2.0,\n",
    "                'COMFORT_LEVEL': 2.0,\n",
    "                'INTERFACE_QUALITY': 2.0,\n",
    "                'SUPPORT_QUALITY': 1.8,\n",
    "                'MIC_NOISE_ISSUE': 2.2,\n",
    "                'VALUE_PERCEPTION': 1.9,\n",
    "                'UPDATE_RESULT': 2.3,\n",
    "                'DEPENDENCY': 1.5,\n",
    "                'CONTEXTUAL': 1.3,\n",
    "                'COMPOUND': 1.4,\n",
    "                'LINGUISTIC': 1.6\n",
    "            }\n",
    "            score *= type_multipliers.get(phrase_type, 1.0)\n",
    "            \n",
    "            # Word count bonus\n",
    "            word_count = len(phrase.split())\n",
    "            if word_count == 2:\n",
    "                score *= 1.3  # Optimal length\n",
    "            elif word_count == 3:\n",
    "                score *= 1.1\n",
    "            \n",
    "            # Aspect coverage bonus\n",
    "            covered_aspects = []\n",
    "            for aspect, terms in self.product_aspects.items():\n",
    "                if any(term in phrase.lower() for term in terms):\n",
    "                    covered_aspects.append(aspect)\n",
    "            \n",
    "            if covered_aspects:\n",
    "                score *= (1 + 0.2 * len(covered_aspects))\n",
    "            \n",
    "            # Sentiment clarity bonus\n",
    "            for sentiment, words in self.sentiment_modifiers.items():\n",
    "                if any(word in phrase.lower() for word in words):\n",
    "                    score *= 1.2\n",
    "                    break\n",
    "            \n",
    "            phrase_scores[phrase] = score\n",
    "        \n",
    "        return phrase_scores\n",
    "    \n",
    "    def _extract_diverse_phrases(self, phrase_scores, n):\n",
    "        \"\"\"Extract diverse phrases covering different aspects\"\"\"\n",
    "        sorted_phrases = sorted(phrase_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        final_phrases = []\n",
    "        covered_aspects = set()\n",
    "        \n",
    "        for phrase, score in sorted_phrases:\n",
    "            if len(final_phrases) >= n:\n",
    "                break\n",
    "            \n",
    "            # Check if this phrase covers a new aspect\n",
    "            phrase_aspects = set()\n",
    "            for aspect, terms in self.product_aspects.items():\n",
    "                if any(term in phrase.lower() for term in terms):\n",
    "                    phrase_aspects.add(aspect)\n",
    "            \n",
    "            # Add if it covers new aspects or is very high scoring\n",
    "            if not phrase_aspects.intersection(covered_aspects) or score > 5.0:\n",
    "                final_phrases.append(phrase)\n",
    "                covered_aspects.update(phrase_aspects)\n",
    "        \n",
    "        # If we don't have enough diverse phrases, fill with highest scoring ones\n",
    "        while len(final_phrases) < n and sorted_phrases:\n",
    "            phrase, _ = sorted_phrases.pop(0)\n",
    "            if phrase not in final_phrases:\n",
    "                final_phrases.append(phrase)\n",
    "        \n",
    "        return final_phrases\n",
    "    \n",
    "    def _clean_phrase(self, phrase):\n",
    "        \"\"\"Clean and normalize a phrase\"\"\"\n",
    "        if not phrase:\n",
    "            return None\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
    "        \n",
    "        # Remove artifacts\n",
    "        phrase = phrase.replace('_', ' ')\n",
    "        \n",
    "        # Remove leading/trailing punctuation\n",
    "        phrase = phrase.strip(self.punc)\n",
    "        \n",
    "        # Skip if too short or just stopwords\n",
    "        words = phrase.split()\n",
    "        if len(words) < 2 or all(w in self.stop_words for w in words):\n",
    "            return None\n",
    "        \n",
    "        # Remove phrases starting or ending with filter words\n",
    "        if words[0] in self.filter_words or words[-1] in self.filter_words:\n",
    "            return None\n",
    "        \n",
    "        return phrase\n",
    "    \n",
    "    def train(self, dataframe):\n",
    "        \"\"\"Train the model by computing IDF values\"\"\"\n",
    "        print(\"Training advanced model...\")\n",
    "        \n",
    "        vocabulary = set()\n",
    "        doc_f = defaultdict(lambda: 0)\n",
    "        \n",
    "        for i, row in dataframe.iterrows():\n",
    "            cleaned_review = self._text_preprocess_clean(row['all_reviews'])\n",
    "            vocabulary.update(cleaned_review)\n",
    "            \n",
    "            unique_words = set(cleaned_review)\n",
    "            for word in unique_words:\n",
    "                doc_f[word] += 1\n",
    "        \n",
    "        DOC_COUNT = len(dataframe)\n",
    "        for word in vocabulary:\n",
    "            self.idf[word] = math.log10(DOC_COUNT / float(doc_f[word]))\n",
    "        \n",
    "        print(f\"Model trained on {DOC_COUNT} documents\")\n",
    "        print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _text_preprocess_clean(self, review):\n",
    "        \"\"\"Clean and tokenize text for IDF calculation\"\"\"\n",
    "        review = re.sub(r'[^\\w\\s]', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = re.sub(r'\\s+', ' ', review)\n",
    "        \n",
    "        tokens = word_tokenize(review)\n",
    "        tokens = [t for t in tokens if len(t) > 2 and not t.isdigit()]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        \"\"\"Save the trained model to disk\"\"\"\n",
    "        if path is None:\n",
    "            path = self.model_path\n",
    "            \n",
    "        model_data = {\n",
    "            'idf': self.idf,\n",
    "            'stop_words': self.stop_words,\n",
    "            'filter_words': self.filter_words,\n",
    "            'punc': self.punc,\n",
    "            'fullstop': self.fullstop,\n",
    "            'semantic_patterns': self.semantic_patterns,\n",
    "            'product_aspects': self.product_aspects,\n",
    "            'sentiment_modifiers': self.sentiment_modifiers,\n",
    "            'parameters': {\n",
    "                'SUMMARY_SIZE_FACTOR': self.SUMMARY_SIZE_FACTOR,\n",
    "                'RF_WEIGHT': self.RF_WEIGHT,\n",
    "                'LEVENSHTEIN_THRESHOLD': self.LEVENSHTEIN_THRESHOLD\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path=\"advanced_review_model.pkl\"):\n",
    "        \"\"\"Load a trained model from disk\"\"\"\n",
    "        model = cls(model_path=path)\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model.idf = model_data['idf']\n",
    "        model.stop_words = model_data['stop_words']\n",
    "        model.filter_words = model_data['filter_words']\n",
    "        model.punc = model_data['punc']\n",
    "        model.fullstop = model_data['fullstop']\n",
    "        model.semantic_patterns = model_data['semantic_patterns']\n",
    "        model.product_aspects = model_data['product_aspects']\n",
    "        model.sentiment_modifiers = model_data['sentiment_modifiers']\n",
    "        \n",
    "        # Recompile patterns\n",
    "        model.compiled_patterns = [(re.compile(pattern, re.IGNORECASE), label) \n",
    "                                  for pattern, label in model.semantic_patterns]\n",
    "        \n",
    "        params = model_data['parameters']\n",
    "        model.SUMMARY_SIZE_FACTOR = params['SUMMARY_SIZE_FACTOR']\n",
    "        model.RF_WEIGHT = params['RF_WEIGHT']\n",
    "        model.LEVENSHTEIN_THRESHOLD = params['LEVENSHTEIN_THRESHOLD']\n",
    "        \n",
    "        print(f\"Model loaded from {path}\")\n",
    "        print(f\"Vocabulary size: {len(model.idf)}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    import utils\n",
    "    \n",
    "    # Training phase (one-time)\n",
    "    print(\"Loading training data...\")\n",
    "    file2 = os.path.join(\"data\", 'asin_numreviews_allreview.csv')\n",
    "    df_allreview = utils.csv_to_dataframe(file2)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = AdvancedReviewSummarizerModel()\n",
    "    model.train(df_allreview)\n",
    "    model.save()\n",
    "    \n",
    "    # Load trained model (for future use)\n",
    "    loaded_model = AdvancedReviewSummarizerModel.load()\n",
    "    \n",
    "    # Summarize new product reviews\n",
    "    new_reviews = [\n",
    "        \"This phone case is really good! It's durable and protective.\",\n",
    "        \"I love the design, but it's a bit bulky for my pocket.\",\n",
    "        \"Great value for money. The screen protector works perfectly.\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    summary = loaded_model.summarize_new_product(new_reviews)\n",
    "    print(\"\\nSummary for new product:\")\n",
    "    for phrase in summary:\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421575a7-4229-4c43-b4e0-24eb257503c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
